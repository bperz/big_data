{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resources/summa.txt\", encoding=\"utf8\") as myfile:\n",
    "    file1 = \" \".join(line.rstrip() for line in myfile)\n",
    "    \n",
    "with open(\"resources/summa2.txt\", encoding=\"utf8\") as myfile:\n",
    "    file2 = \" \".join(line.rstrip() for line in myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    text = re.sub(r\"[^a-zA-Z]+\", \" \", data)\n",
    "    text = re.sub(r\"^.*?(?=(QUESTION))\", \"\", text)\n",
    "    text = re.sub(r\"(?<=(FOR EVER BLESSED Amen)).+\", '', text)\n",
    "    text = re.sub(r\"ON FATE\", \"QUESTION\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def chapters_split(data):\n",
    "    chapters = []\n",
    "    chap = []\n",
    "\n",
    "    cleaned = clean(data).split(\" \")\n",
    "\n",
    "    for i, word in enumerate(cleaned):\n",
    "\n",
    "        if (word == \"QUESTION\") or (i==len(cleaned)-1):\n",
    "            chapters.append(' '.join(chap))\n",
    "            chap = []\n",
    "\n",
    "        else:\n",
    "            chap.append(word)\n",
    "\n",
    "    chapters = [chap for chap in chapters[1:]]\n",
    "    \n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(data, stopWords):\n",
    "    text = re.sub(r\"[^\\w]+\", \" \", data).split(\" \")\n",
    "    \n",
    "    words = [word.lower() for word in text if word not in stopWords and len(word) > 1 and word[0] != \"'\"]\n",
    "                \n",
    "    return words\n",
    "\n",
    "\n",
    "def shingles(file1, k, stopWords):\n",
    "    \n",
    "    f1_words = text_split(file1, stopWords)\n",
    "    \n",
    "    f1_shingles = set(tuple(f1_words[i:i+k]) for i in range(len(f1_words)-k+1))\n",
    "    \n",
    "    return f1_shingles\n",
    "\n",
    "\n",
    "def jaccard(f1, f2, k, stopWords):\n",
    "    \n",
    "    f1_shingles = shingles(f1, k, stopWords)\n",
    "    f2_shingles = shingles(f2, k, stopWords)\n",
    "    \n",
    "    intersection = f1_shingles.intersection(f2_shingles)\n",
    "    union = f1_shingles.union(f2_shingles)\n",
    "        \n",
    "    return 1 - len(intersection)/len(union)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9959429683339193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = clean(file1)\n",
    "f2 = clean(file2)\n",
    "\n",
    "jaccard(f1, f2, 4, stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = chapters_split(file1)\n",
    "# stopWords = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "JDDS = []\n",
    "for i in range(len(chapters)-1):\n",
    "    JDDS.append([])\n",
    "    for j in range(i+1,len(chapters)):\n",
    "        JD = jaccard(chapters[i], chapters[j], 7, stopWords)\n",
    "        JDDS[i].append(JD)\n",
    "#         print(\"Chapters {} and {} distance: {}\".format(i, j, JD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_shingles(chapters, k, stopWords):\n",
    "    all_set = set()\n",
    "    \n",
    "    for chap in chapters:\n",
    "        all_set.update(shingles(chap, k, stopWords))\n",
    "        \n",
    "    return sorted(list(all_set))\n",
    "\n",
    "\n",
    "def create_hash_family(all_shingles, n):\n",
    "    length = len(all_shingles)\n",
    "    \n",
    "    perms = [np.random.permutation(length) for i in range(n)]\n",
    "    \n",
    "    return perms\n",
    "\n",
    "\n",
    "def signatures(shingles1, shingles2, all_shingles, hash_family):\n",
    "    \n",
    "    signatures = np.zeros((len(hash_family), 2))\n",
    "    \n",
    "    for i, c_hash in enumerate(hash_family):\n",
    "        s1_ready = s2_ready = False\n",
    "        \n",
    "        for j, index in enumerate(c_hash):\n",
    "            if s1_ready == False and all_shingles[index] in shingles1:\n",
    "                signatures[i,0] = j+1\n",
    "                s1_ready = True\n",
    "                \n",
    "            if s2_ready == False and all_shingles[index] in shingles2:\n",
    "                signatures[i,1] = j+1\n",
    "                s2_ready = True\n",
    "                \n",
    "            if s1_ready and s2_ready:\n",
    "                break\n",
    "                \n",
    "    return signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shingles = create_all_shingles(chapters, 7, stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hash = 10000\n",
    "hash_family = create_hash_family(all_shingles, n_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shingle1, shingle2) = (shingles(chapters[99], 7, stopWords), shingles(chapters[100], 7, stopWords))\n",
    "\n",
    "sig = signatures(shingle1, shingle2, all_shingles, hash_family)\n",
    "sim = 1 - np.count_nonzero((sig[:,0] == sig[:,1]))/np.count_nonzero((sig[:,0] + sig[:,1] > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922680412371134"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JDDS[99][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
